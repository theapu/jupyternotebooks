{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a641bf3",
   "metadata": {},
   "source": [
    "# Check NVIDIA Driver version and python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b1926-4429-43ab-9e0f-78202f97492f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed48f21-d65d-4c53-b0c5-c1005ee1780e",
   "metadata": {},
   "source": [
    "# Install apt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a637daf-bc40-4895-840e-6100343e61c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y build-essential cmake curl libssl-dev libcurl4-openssl-dev unzip pciutils libgl1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd9047d",
   "metadata": {},
   "source": [
    "# Install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0bd776-a031-4f67-a15b-407f9b9a491c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a127b4d",
   "metadata": {},
   "source": [
    "# Load Model using unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2664b0-a6e9-452d-a7d7-5e519fd3d071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# # 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "# fourbit_models = [\n",
    "#     \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "#     \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "#     \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "#     \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "#     \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "#     \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "#     \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "#     \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "#     \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "#     \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "#     \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "#     \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "#     \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "#     \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "\n",
    "#     \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
    "# ] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2503ed9",
   "metadata": {},
   "source": [
    "# Add LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7effd083-62e3-4464-bee6-1acbe2c470fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e432a1",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342dc19d-0a96-411c-85dc-0210af7e63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"/dataset.jsonl\", split=\"train\")\n",
    "\n",
    "print(dataset[0])  # Check First entry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f623128a",
   "metadata": {},
   "source": [
    "# Map to llama chat format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d93c7-f0aa-4123-8985-488d821e64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "#Show message in lama format\n",
    "dataset[5][\"messages\"]\n",
    "#Show text of message\n",
    "dataset[5][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637db9e3",
   "metadata": {},
   "source": [
    "# Define trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed797e-abfc-4ec2-b56c-f2e968665cfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339fae6-f393-4677-8504-30e64baf4cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f51b0a",
   "metadata": {},
   "source": [
    "# Verify masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31945cd7-958c-43cf-a498-22fb693fc251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])\n",
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882674dc",
   "metadata": {},
   "source": [
    "# Start training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415877a-84fa-4faf-8235-dd53fd3921f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ceeb9-ace6-4fd4-b46e-854c01dfaca7",
   "metadata": {},
   "source": [
    "# GPU stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad46e0-0ff5-4752-ac48-cea59bbf3783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get the total GPU memory\n",
    "max_memory = torch.cuda.get_device_properties(0).total_memory / 1024 / 1024 / 1024  # GB\n",
    "\n",
    "# Calculate memory usage\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "start_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024 / 1024  # Convert to GB\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "# Print stats\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385ccb3",
   "metadata": {},
   "source": [
    "# Test trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689244b-6cc8-4d78-8265-30f5ea893f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "import re\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Convert to latex bibliography @article{Low_complexity_QSM,\\n  serialno={25},\\n  author={Z. {Yigit} and E. {Basar}},\\n  year={2016},\\n  title={Low-complexity detection of quadrature spatial modulation},\\n  journal={\\\\rvtEleLett},\\n  volume={52},\\n  number={20},\\n  pages={1729--1731},\\n  doi={10.1049/el.2016.1583},\\n}\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "attention_mask = inputs != tokenizer.pad_token_id\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 256, use_cache = True, \n",
    "                         attention_mask=attention_mask, \n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "\n",
    "#tokenizer.batch_decode(outputs)\n",
    "\n",
    "# Decode the generated tokens into human-readable text\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "def extract_assistant_response(text):\n",
    "    match = re.search(r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>\", text, re.DOTALL)\n",
    "    return match.group(1).strip() if match else None\n",
    "\n",
    "content = extract_assistant_response(text)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ffd9f-9277-464e-a2bd-b1d72854da74",
   "metadata": {},
   "source": [
    "# Test trained model streamed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec988c-8988-445c-9a5b-f98ef0067a23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# Get the token ID for <|eot_id|>\n",
    "eot_token_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "# Custom streamer to skip <|eot_id|>\n",
    "class CustomStreamer(TextStreamer):\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        if text.strip().endswith(\"<|eot_id|>\"):\n",
    "            text = text.replace(\"<|eot_id|>\", \"\").strip()  # Remove the token\n",
    "            super().on_finalized_text(text, stream_end=True)  # Force stream end\n",
    "        else:\n",
    "            super().on_finalized_text(text, stream_end=stream_end)\n",
    "\n",
    "# Enable faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Convert to latex bibliography @article{Low_complexity_QSM,\\n  serialno={25},\\n  author={Z. {Yigit} and E. {Basar}},\\n  year={2016},\\n  title={Low-complexity detection of quadrature spatial modulation},\\n  journal={\\\\rvtEleLett},\\n  volume={52},\\n  number={20},\\n  pages={1729--1731},\\n  doi={10.1049/el.2016.1583},\\n}\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "attention_mask = inputs != tokenizer.pad_token_id\n",
    "\n",
    "text_streamer = CustomStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Generate with custom stopping and cleanup\n",
    "_ = model.generate(\n",
    "    input_ids=inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=256,\n",
    "    attention_mask=attention_mask,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1,\n",
    "    eos_token_id=eot_token_id,  # Stop generation at <|eot_id|>\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482ddb1-eec6-4b7d-ae6e-398001671a77",
   "metadata": {},
   "source": [
    "# Save model as LoRA adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225006da-6372-4361-90ca-32c895a02e8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"bibtex_lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"bibtex_lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d3f121-4535-4381-877d-d2e8ca7dbf1d",
   "metadata": {},
   "source": [
    "# Test inference using saved lora models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e010486a-5227-4a7a-8837-2e185609bf27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the token ID for <|eot_id|>\n",
    "eot_token_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "# Custom streamer to skip <|eot_id|>\n",
    "class CustomStreamer(TextStreamer):\n",
    "    def on_finalized_text(self, text: str, stream_end: bool = False):\n",
    "        if text.strip().endswith(\"<|eot_id|>\"):\n",
    "            text = text.replace(\"<|eot_id|>\", \"\").strip()  # Remove the token\n",
    "            super().on_finalized_text(text, stream_end=True)  # Force stream end\n",
    "        else:\n",
    "            super().on_finalized_text(text, stream_end=stream_end)\n",
    "            \n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"bibtex_lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Convert to latex bibliography @article{Low_complexity_QSM,\\n  serialno={25},\\n  author={Z. {Yigit} and E. {Basar}},\\n  year={2016},\\n  title={Low-complexity detection of quadrature spatial modulation},\\n  journal={\\\\rvtEleLett},\\n  volume={52},\\n  number={20},\\n  pages={1729--1731},\\n  doi={10.1049/el.2016.1583},\\n}\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "text_streamer = CustomStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Generate with custom stopping and cleanup\n",
    "_ = model.generate(\n",
    "    input_ids=inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=256,\n",
    "    attention_mask=attention_mask,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1,\n",
    "    eos_token_id=eot_token_id,  # Stop generation at <|eot_id|>\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3753c-6017-4ec6-8d66-825c94dec1a9",
   "metadata": {},
   "source": [
    "# Save as GGUF / llama.cpp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101f9b0e-8f01-4dff-a491-cc25198ddb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "model.save_pretrained_gguf(\"bibtex_model\", tokenizer,)\n",
    "# if False: model.save_pretrained_gguf(\"bibtex_model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "# if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# # Save to 16bit GGUF\n",
    "# if False: model.save_pretrained_gguf(\"bibtex_model\", tokenizer, quantization_method = \"f16\")\n",
    "# # if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "# model.save_pretrained_gguf(\"bibtex_model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "#if False: model.save_pretrained_gguf(\"bibtex_model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "# if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# # Save to multiple GGUF options - much faster if you want multiple!\n",
    "# if False:\n",
    "#     model.push_to_hub_gguf(\n",
    "#         \"hf/model\", # Change hf to your username!\n",
    "#         tokenizer,\n",
    "#         quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "#         token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f909b3-9d30-4f4b-bd6d-9f3cfa027f06",
   "metadata": {},
   "source": [
    "# Troubleshooting for quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a7ee6-1186-454b-a319-4db982bbe1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get RuntimeError: Unsloth: The file 'llama.cpp/llama-quantize' or 'llama.cpp/quantize' does not exist run this block.\n",
    "# Then copy llama.cpp/build/bin/llama-quantize to llama.cpp/ and run the block above again.\n",
    "!(cd llama.cpp; cmake -B build;cmake --build build --config Release)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d723837-860e-41cb-8c78-b489a80bdfbe",
   "metadata": {},
   "source": [
    "# Install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b872117-fe69-42a5-8dc7-22bfc96a9941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!echo \"-= Done. =-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d281f-7422-489a-af41-ea54466147c8",
   "metadata": {},
   "source": [
    "# Install openweb ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53b126-47ee-40aa-9930-5f33bf315001",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install open-webui\n",
    "!pip install ffmpeg\n",
    "!echo \"-= Done. =-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e44b69d-6817-442f-9b52-ca2463f43e19",
   "metadata": {},
   "source": [
    "# Start ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d05ab62-3cc2-4a69-802a-de679d0dc3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "log_file = \"ollama.log\"\n",
    "with open(log_file, \"w\") as f:\n",
    "    subprocess.Popen([\"ollama\", \"serve\"], stdout=f, stderr=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dbba89-58b0-4898-9e0d-249cbb5bf87e",
   "metadata": {},
   "source": [
    "# Show ollama logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe236c-bd9b-4baf-a2f3-e88c3a931b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ollama.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4338323-6b0c-4f7d-a929-35e2ba4f04e3",
   "metadata": {},
   "source": [
    "# Add our finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bad71a1-66b7-4d2a-b3e6-9056795eeb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama create bibtexmodel -f /bibtex_model/Modelfile\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a78b20f-fa44-4596-9641-9cc35dfba28b",
   "metadata": {},
   "source": [
    "# Install cloudflare tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3f24c-c3f3-48b0-aa5b-ca1f2079937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install cloudflare\n",
    "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
    "!dpkg -i cloudflared-linux-amd64.deb\n",
    "!rm -rf cloudflared-linux-amd64.deb\n",
    "!echo \"-= Done. =-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3cc2f-1f10-4128-b5b1-f09c29484112",
   "metadata": {},
   "source": [
    "# Start cloudflare tunnel and openweb ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f321251d-424a-4d25-a983-13dcd4377bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Start cloudflare tunnel and openwebui\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import socket\n",
    "import urllib.request\n",
    "\n",
    "OPENWEBUI_PORT = 9999\n",
    "\n",
    "def iframe_thread(port):\n",
    "  while True:\n",
    "      time.sleep(0.5)\n",
    "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "      result = sock.connect_ex(('127.0.0.1', port))\n",
    "      if result == 0:\n",
    "        break\n",
    "      sock.close()\n",
    "  print(\"\\nComfyUI finished loading, trying to launch cloudflared (if it gets stuck here cloudflared is having issues)\\n\")\n",
    "\n",
    "  p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", \"http://127.0.0.1:{}\".format(port)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "  for line in p.stderr:\n",
    "    l = line.decode()\n",
    "    if \"trycloudflare.com \" in l:\n",
    "      cf_url = l[l.find(\"http\"):]\n",
    "      print(\"This is the URL to access ComfyUI:\", cf_url, end='')\n",
    "\n",
    "threading.Thread(target=iframe_thread, daemon=True, args=(OPENWEBUI_PORT,)).start()\n",
    "\n",
    "!open-webui serve --port $OPENWEBUI_PORT\n",
    "\n",
    "!echo \"-= Done. =-\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
